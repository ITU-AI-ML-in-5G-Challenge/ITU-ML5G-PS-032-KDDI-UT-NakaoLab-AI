{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "import numpy as np\n",
    "from numpy import sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost(X_train, y_train, X_test, y_test):\n",
    "    last_time = time.time()\n",
    "    xgb = XGBClassifier(n_estimators=300, objective='multi:softmax', num_class=13, random_state=0)\n",
    "\n",
    "    xgb.fit(X_train, y_train)\n",
    "    middle_time = time.time()\n",
    "\n",
    "    y_pred = xgb.predict(X_test)\n",
    "\n",
    "    current_time = time.time()\n",
    "\n",
    "    accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(\"训练耗时： {}\".format(middle_time - last_time))\n",
    "    print(\"测试耗时： {}\".format(current_time - middle_time))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print('confusion matrix xgb:')\n",
    "    print(cm)\n",
    "    print('classification report xgb:')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    plot_confusion_matrix(cm, classes=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13'],\n",
    "                          normalize=True, title='Normalized confusion matrix')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.4f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"red\")\n",
    "        # color=\"red\" if cm[i, j] > thresh else \"black\")\n",
    "    # plt.set_tight_layout(True)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plot_confusion_matrix(cm, classes=['0', '1', '2'], normalize=True, title='Normalized confusion matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_csv network: /Users/xiafei/Downloads/csv-for-learning/20200629.n.csv\n",
      "read_csv network: /Users/xiafei/Downloads/csv-for-learning/20200630.n.csv\n",
      "read_csv network: /Users/xiafei/Downloads/csv-for-learning/20200701.n.csv\n",
      "read_csv network: /Users/xiafei/Downloads/csv-for-learning/20200702.n.csv\n",
      "read_csv network: /Users/xiafei/Downloads/csv-for-learning/20200703.n.csv\n",
      "read_csv network: /Users/xiafei/Downloads/csv-for-learning/20200704.n.csv\n",
      "read_csv network: /Users/xiafei/Downloads/csv-for-learning/20200705.n.csv\n",
      "read_csv network: /Users/xiafei/Downloads/csv-for-learning/20200706.n.csv\n",
      "read_csv virtual: /Users/xiafei/Downloads/csv-for-learning/20200629.v.csv\n",
      "read_csv virtual: /Users/xiafei/Downloads/csv-for-learning/20200630.v.csv\n",
      "read_csv virtual: /Users/xiafei/Downloads/csv-for-learning/20200701.v.csv\n",
      "read_csv virtual: /Users/xiafei/Downloads/csv-for-learning/20200702.v.csv\n",
      "read_csv virtual: /Users/xiafei/Downloads/csv-for-learning/20200703.v.csv\n",
      "read_csv virtual: /Users/xiafei/Downloads/csv-for-learning/20200704.v.csv\n",
      "read_csv virtual: /Users/xiafei/Downloads/csv-for-learning/20200705.v.csv\n",
      "read_csv virtual: /Users/xiafei/Downloads/csv-for-learning/20200706.v.csv\n",
      "read_csv physical: /Users/xiafei/Downloads/csv-for-learning/20200629.p.csv\n",
      "read_csv physical: /Users/xiafei/Downloads/csv-for-learning/20200630.p.csv\n",
      "read_csv physical: /Users/xiafei/Downloads/csv-for-learning/20200701.p.csv\n",
      "read_csv physical: /Users/xiafei/Downloads/csv-for-learning/20200702.p.csv\n",
      "read_csv physical: /Users/xiafei/Downloads/csv-for-learning/20200703.p.csv\n",
      "read_csv physical: /Users/xiafei/Downloads/csv-for-learning/20200704.p.csv\n",
      "read_csv physical: /Users/xiafei/Downloads/csv-for-learning/20200705.p.csv\n",
      "read_csv physical: /Users/xiafei/Downloads/csv-for-learning/20200706.p.csv\n",
      "(9670, 271)\n",
      "(9670, 655)\n",
      "(9670, 146)\n",
      "列数: 1068 行数: 9670\n",
      "(7736, 1066) (7736,)\n",
      "(1934, 1066) (1934,)\n"
     ]
    }
   ],
   "source": [
    "    # 显示所有列\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    # 显示所有行\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    # 设置value的显示长度为100，默认为50\n",
    "    pd.set_option('max_colwidth', 100)\n",
    "\n",
    "    # 读取csv\n",
    "    # path = r'/home/itu/datadisk/dataset/csv-for-learning/'\n",
    "    path = r'/Users/xiafei/Downloads/csv-for-learning/'\n",
    "    # all_n_files = glob.glob(path + \"/*.n.csv\")\n",
    "    # all_v_files = glob.glob(path + \"/*.v.csv\")\n",
    "    # all_p_files = glob.glob(path + \"/*.p.csv\")\n",
    "    all_n_files = [path + x for x in\n",
    "                   ['20200629.n.csv', '20200630.n.csv', '20200701.n.csv', '20200702.n.csv', '20200703.n.csv',\n",
    "                    '20200704.n.csv', '20200705.n.csv', '20200706.n.csv']]\n",
    "    all_v_files = [path + x for x in\n",
    "                   ['20200629.v.csv', '20200630.v.csv', '20200701.v.csv', '20200702.v.csv', '20200703.v.csv',\n",
    "                    '20200704.v.csv', '20200705.v.csv', '20200706.v.csv']]\n",
    "    all_p_files = [path + x for x in\n",
    "                   ['20200629.p.csv', '20200630.p.csv', '20200701.p.csv', '20200702.p.csv', '20200703.p.csv',\n",
    "                    '20200704.p.csv', '20200705.p.csv', '20200706.p.csv']]\n",
    "\n",
    "    li_n = []\n",
    "    li_v = []\n",
    "    li_p = []\n",
    "\n",
    "    for filename in all_n_files:\n",
    "        print('read_csv network:', filename)\n",
    "        df = pd.read_csv(filename, index_col=None, header=0)\n",
    "        li_n.append(df)\n",
    "\n",
    "    for filename in all_v_files:\n",
    "        print('read_csv virtual:', filename)\n",
    "        df = pd.read_csv(filename, index_col=None, header=0)\n",
    "        li_v.append(df)\n",
    "\n",
    "    for filename in all_p_files:\n",
    "        print('read_csv physical:', filename)\n",
    "        df = pd.read_csv(filename, index_col=None, header=0)\n",
    "        li_p.append(df)\n",
    "\n",
    "    dataset_n = pd.concat(li_n, axis=0, ignore_index=True, sort=False)\n",
    "    dataset_v = pd.concat(li_v, axis=0, ignore_index=True, sort=False)\n",
    "    dataset_p = pd.concat(li_p, axis=0, ignore_index=True, sort=False)\n",
    "\n",
    "    print(dataset_n.shape)\n",
    "    print(dataset_v.shape)\n",
    "    print(dataset_p.shape)\n",
    "\n",
    "    dataset_n.drop(['type', 'type_code'], axis=1, inplace=True)\n",
    "    dataset_v.drop(['type', 'type_code'], axis=1, inplace=True)\n",
    "\n",
    "    dataset_n.rename(columns=lambda x: 'n_' + x, inplace=True)\n",
    "    dataset_v.rename(columns=lambda x: 'v_' + x, inplace=True)\n",
    "    dataset_p.rename(columns=lambda x: 'p_' + x, inplace=True)\n",
    "\n",
    "    dataset = pd.concat([dataset_n, dataset_v, dataset_p], axis=1, sort=False)\n",
    "    # 数据集概览\n",
    "    # print(dataset.describe())\n",
    "    # print(dataset.head(5))\n",
    "\n",
    "    # valid\n",
    "    # print('isnan', np.isnan(dataset.any()))\n",
    "    # print('isfinite', np.isfinite(dataset.all()))\n",
    "    # dataset = pd.read_csv('/home/itu/datadisk/dataset/csv-for-learning/20200629.n.csv')\n",
    "\n",
    "    print('列数:', dataset.shape[1], '行数:', dataset.shape[0])\n",
    "\n",
    "    column = dataset.columns\n",
    "    X = dataset[column[:-2]]\n",
    "    # X= X.values\n",
    "    Y = dataset[column[-1]]\n",
    "    # Y= Y.values\n",
    "    # 划分训练测试\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "    print(X_train.shape, y_train.shape)\n",
    "    print(X_test.shape, y_test.shape)\n",
    "\n",
    "    ss = StandardScaler()\n",
    "    std_X_train = ss.fit_transform(X_train)\n",
    "    std_X_test = ss.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XGBClassifier(importance_type='gain')\n",
    "model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# show importance image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(20, 100))\n",
    "plot_importance(model, ax=ax, max_num_features=200, importance_type='gain')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write result to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # 结果写入文件\n",
    "im = pd.DataFrame({'importance': model.feature_importances_, 'var': dataset.columns[:-2]})\n",
    "im = im.sort_values(by='importance', ascending=False)\n",
    "im.to_csv(\"feature_important_data_XG.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00161694 0.0016192  0.00163051 0.00164602 0.00165121 0.00165458\n",
      " 0.0016593  0.00166001 0.00166666 0.0016675  0.00167826 0.00168187\n",
      " 0.00168245 0.00168806 0.00169543 0.00170044 0.00170201 0.00170274\n",
      " 0.00170803 0.00171275 0.00171286 0.00174048 0.00174438 0.00175812\n",
      " 0.00177849 0.00178315 0.00181165 0.00182481 0.00184686 0.00186827\n",
      " 0.00188302 0.0018952  0.00189637 0.00189994 0.00193583 0.00200437\n",
      " 0.00200541 0.00201293 0.00201563 0.0020375  0.00215802 0.00216093\n",
      " 0.00216682 0.00217154 0.00222244 0.00222873 0.0022385  0.00225193\n",
      " 0.00230204 0.0023082  0.00230905 0.0023281  0.00236029 0.00239628\n",
      " 0.00241818 0.00246973 0.00252312 0.00253492 0.0025465  0.00261815\n",
      " 0.00295379 0.00296716 0.00311144 0.00317391 0.00339135 0.00346303\n",
      " 0.00359551 0.00393201 0.0041093  0.00417794 0.00444713 0.00448986\n",
      " 0.00506805 0.00531637 0.00539663 0.00639367 0.00708875 0.00744147\n",
      " 0.00800852 0.00884959 0.00894771 0.0089646  0.01010152 0.01024031\n",
      " 0.01124802 0.0118994  0.01225778 0.01707036 0.0204329  0.02167401\n",
      " 0.02177017 0.03546158 0.03937764 0.03977635 0.04273143 0.04377725\n",
      " 0.04382794 0.0587829  0.06344514 0.0887851 ]\n"
     ]
    }
   ],
   "source": [
    "thresholds = sort(model.feature_importances_)[-100:]\n",
    "print(thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thresh=0.0016, n=100, Accuracy: 75.70%\n",
      "Thresh=0.0016, n=99, Accuracy: 75.28%\n",
      "Thresh=0.0016, n=98, Accuracy: 75.34%\n",
      "Thresh=0.0016, n=97, Accuracy: 75.96%\n",
      "Thresh=0.0017, n=96, Accuracy: 75.80%\n",
      "Thresh=0.0017, n=95, Accuracy: 76.27%\n",
      "Thresh=0.0017, n=94, Accuracy: 75.54%\n",
      "Thresh=0.0017, n=93, Accuracy: 75.75%\n",
      "Thresh=0.0017, n=92, Accuracy: 75.49%\n",
      "Thresh=0.0017, n=91, Accuracy: 74.61%\n",
      "Thresh=0.0017, n=90, Accuracy: 75.28%\n",
      "Thresh=0.0017, n=89, Accuracy: 76.16%\n",
      "Thresh=0.0017, n=88, Accuracy: 75.80%\n",
      "Thresh=0.0017, n=87, Accuracy: 75.80%\n",
      "Thresh=0.0017, n=86, Accuracy: 75.80%\n",
      "Thresh=0.0017, n=85, Accuracy: 75.39%\n",
      "Thresh=0.0017, n=84, Accuracy: 75.23%\n",
      "Thresh=0.0017, n=83, Accuracy: 75.65%\n",
      "Thresh=0.0017, n=82, Accuracy: 75.23%\n",
      "Thresh=0.0017, n=81, Accuracy: 75.34%\n",
      "Thresh=0.0017, n=80, Accuracy: 75.28%\n",
      "Thresh=0.0017, n=79, Accuracy: 74.87%\n",
      "Thresh=0.0017, n=78, Accuracy: 75.49%\n",
      "Thresh=0.0018, n=77, Accuracy: 76.16%\n",
      "Thresh=0.0018, n=76, Accuracy: 74.97%\n",
      "Thresh=0.0018, n=75, Accuracy: 75.75%\n",
      "Thresh=0.0018, n=74, Accuracy: 75.13%\n",
      "Thresh=0.0018, n=73, Accuracy: 75.70%\n",
      "Thresh=0.0018, n=72, Accuracy: 75.85%\n",
      "Thresh=0.0019, n=71, Accuracy: 75.75%\n",
      "Thresh=0.0019, n=70, Accuracy: 75.23%\n",
      "Thresh=0.0019, n=69, Accuracy: 76.32%\n",
      "Thresh=0.0019, n=68, Accuracy: 76.58%\n",
      "Thresh=0.0019, n=67, Accuracy: 75.75%\n",
      "Thresh=0.0019, n=66, Accuracy: 75.96%\n",
      "Thresh=0.0020, n=65, Accuracy: 76.11%\n",
      "Thresh=0.0020, n=64, Accuracy: 76.63%\n",
      "Thresh=0.0020, n=63, Accuracy: 75.75%\n",
      "Thresh=0.0020, n=62, Accuracy: 76.11%\n",
      "Thresh=0.0020, n=61, Accuracy: 75.59%\n",
      "Thresh=0.0022, n=60, Accuracy: 76.01%\n",
      "Thresh=0.0022, n=59, Accuracy: 76.11%\n",
      "Thresh=0.0022, n=58, Accuracy: 76.16%\n",
      "Thresh=0.0022, n=57, Accuracy: 76.78%\n",
      "Thresh=0.0022, n=56, Accuracy: 76.58%\n",
      "Thresh=0.0022, n=55, Accuracy: 76.22%\n",
      "Thresh=0.0022, n=54, Accuracy: 75.28%\n",
      "Thresh=0.0023, n=53, Accuracy: 75.96%\n",
      "Thresh=0.0023, n=52, Accuracy: 76.27%\n",
      "Thresh=0.0023, n=51, Accuracy: 76.06%\n",
      "Thresh=0.0023, n=50, Accuracy: 76.47%\n",
      "Thresh=0.0023, n=49, Accuracy: 76.63%\n",
      "Thresh=0.0024, n=48, Accuracy: 75.90%\n",
      "Thresh=0.0024, n=47, Accuracy: 76.73%\n",
      "Thresh=0.0024, n=46, Accuracy: 76.37%\n",
      "Thresh=0.0025, n=45, Accuracy: 75.85%\n",
      "Thresh=0.0025, n=44, Accuracy: 76.06%\n",
      "Thresh=0.0025, n=43, Accuracy: 76.37%\n",
      "Thresh=0.0025, n=42, Accuracy: 76.94%\n",
      "Thresh=0.0026, n=41, Accuracy: 76.22%\n",
      "Thresh=0.0030, n=40, Accuracy: 75.03%\n",
      "Thresh=0.0030, n=39, Accuracy: 77.87%\n",
      "Thresh=0.0031, n=38, Accuracy: 76.68%\n",
      "Thresh=0.0032, n=37, Accuracy: 76.68%\n",
      "Thresh=0.0034, n=36, Accuracy: 77.25%\n",
      "Thresh=0.0035, n=35, Accuracy: 78.08%\n",
      "Thresh=0.0036, n=34, Accuracy: 78.18%\n",
      "Thresh=0.0039, n=33, Accuracy: 78.49%\n",
      "Thresh=0.0041, n=32, Accuracy: 78.80%\n",
      "Thresh=0.0042, n=31, Accuracy: 79.89%\n",
      "Thresh=0.0044, n=30, Accuracy: 78.80%\n",
      "Thresh=0.0045, n=29, Accuracy: 80.40%\n",
      "Thresh=0.0051, n=28, Accuracy: 80.46%\n",
      "Thresh=0.0053, n=27, Accuracy: 81.28%\n",
      "Thresh=0.0054, n=26, Accuracy: 79.47%\n",
      "Thresh=0.0064, n=25, Accuracy: 81.70%\n",
      "Thresh=0.0071, n=24, Accuracy: 80.77%\n",
      "Thresh=0.0074, n=23, Accuracy: 80.61%\n",
      "Thresh=0.0080, n=22, Accuracy: 80.51%\n",
      "Thresh=0.0088, n=21, Accuracy: 79.52%\n",
      "Thresh=0.0089, n=20, Accuracy: 81.90%\n",
      "Thresh=0.0090, n=19, Accuracy: 81.85%\n",
      "Thresh=0.0101, n=18, Accuracy: 83.09%\n",
      "Thresh=0.0102, n=17, Accuracy: 82.37%\n",
      "Thresh=0.0112, n=16, Accuracy: 81.80%\n",
      "Thresh=0.0119, n=15, Accuracy: 82.32%\n",
      "Thresh=0.0123, n=14, Accuracy: 83.35%\n",
      "Thresh=0.0171, n=13, Accuracy: 84.13%\n",
      "Thresh=0.0204, n=12, Accuracy: 83.61%\n",
      "Thresh=0.0217, n=11, Accuracy: 83.35%\n",
      "Thresh=0.0218, n=10, Accuracy: 82.47%\n",
      "Thresh=0.0355, n=9, Accuracy: 84.33%\n",
      "Thresh=0.0394, n=8, Accuracy: 83.40%\n",
      "Thresh=0.0398, n=7, Accuracy: 82.89%\n",
      "Thresh=0.0427, n=6, Accuracy: 83.51%\n",
      "Thresh=0.0438, n=5, Accuracy: 82.89%\n",
      "Thresh=0.0438, n=4, Accuracy: 83.97%\n",
      "Thresh=0.0588, n=3, Accuracy: 56.20%\n",
      "Thresh=0.0634, n=2, Accuracy: 54.14%\n",
      "Thresh=0.0888, n=1, Accuracy: 25.39%\n"
     ]
    }
   ],
   "source": [
    "for thresh in thresholds:\n",
    "    # select features using threshold\n",
    "    selection = SelectFromModel(model, threshold=thresh, prefit=True)\n",
    "    select_X_train = selection.transform(X_train)\n",
    "    # train model\n",
    "    selection_model = XGBClassifier()\n",
    "    selection_model.fit(select_X_train, y_train)\n",
    "    # eval model\n",
    "    select_X_test = selection.transform(X_test)\n",
    "    y_pred = selection_model.predict(select_X_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(\"Thresh=%.4f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1], accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
